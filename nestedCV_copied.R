### nested CV

#' @param outer_method String of either `"cv"`, `"LOOCV"` or `'SKFold'` specifying whether
#'   to do k-fold CV or leave one out CV (LOOCV) for the outer folds
#' @param n_outer_folds Number of outer CV folds
#' @param outer_folds Optional list containing indices of test folds for outer
#'   CV. If supplied, `n_outer_folds` is ignored.
#' @param metric A string that specifies what summary metric will be used to
#'   select the optimal model. By default, "logLoss" is used for classification
#'   and "RMSE" is used for regression. Note this differs from the default
#'   setting in caret which uses "Accuracy" for classification. See details.
#' @param trControl A list of values generated by the `caret` function
#'   [trainControl]. This defines how inner CV training through `caret` is
#'   performed. Default for the inner loop is 10-fold CV. See
#'   http://topepo.github.io/caret/using-your-own-model-in-train.html.
#' @param cv.cores Number of cores for parallel processing of the outer loops.
#'   NOTE: this uses `parallel::mclapply` on unix/mac and `parallel::parLapply`
#'   on windows.
#' @param finalCV Logical whether to perform one last round of CV on the whole
#'   dataset to determine the final model parameters. If set to `FALSE`, the
#'   median of the best hyperparameters from outer CV folds for continuous/
#'   ordinal hyperparameters, or highest voted for categorical hyperparameters,
#'   are used to fit the final model. Performance metrics are independent of
#'   this last step.
#'   
nestcv_train <- function(class,
                         train_data,
                         outer_method, 
                         n_outer_folds = 10,
                         outer_repeatize = 10,
                         cv.cores,
                         learner,
                         fittingParams,
                         resamplingParams,
                         VariablesearchAlgorithm,
                         combined_variables,
                         clinical_variables_names,
                         TMB_variables_names,
                         finalCV = TRUE,
                         nested_features_selection_search = TRUE
                         ) {
  
  
  
  #### balance samples due to small size  
  # resmaple_out <- randomsample(y = train_data[,class], 
  #                              x = train_data[, -which(names(train_data) %in% c(class))]  )
  # 
  # resmaple_y2 <- as.data.frame(resmaple_out$y)
  # colnames(resmaple_y2) = class
  # resmaple_x2 <- resmaple_out$x
  # resmaple_train_data = cbind(resmaple_y2, resmaple_x2)
  
  
  resmaple_train_data =  train_data
  ###############
  column.names = names(resmaple_train_data)
  class.position <- which(column.names == class) 
  
  #####################
  metric = fittingParams$metric
  samplingmethod = resamplingParams$method
  numberFOLD = resamplingParams$number
  numberFoldRepeats = resamplingParams$repeats
  searchMethod = resamplingParams$search
  
  
  set.seed(1234)
  
  ## outer CV  
  outer_folds <- switch(outer_method,
                        cv = createFolds(resmaple_train_data$response, k = n_outer_folds),
                        LOOCV = 1:length(resmaple_train_data$response),
                        SKFold = createMultiFolds(resmaple_train_data$response, 
                                                  k = n_outer_folds,
                                                  times = outer_repeatize) 
                        )
  
  ############################## run parralel codel
  # for feature selection
  dots <- list()
  
  varlist <- c("class",
               "train_data", 
               "outer_folds", 
               "metric",
               "samplingmethod",
               "numberFOLD", 
               "numberFoldRepeats", 
               "searchMethod",
               "outer_method", 
               "learner",
               'VariablesearchAlgorithm',
               'combined_variables',
               'clinical_variables_names',
               'TMB_variables_names',
               'nested_features_selection_search',
               "nestcv_innertrain", "dots")
  
  
  parallelInnerNested =   function(testIndices) {
    
    args <- c(list(class = class,
                   train_data = resmaple_train_data,
                   testIndices = testIndices,
                   metric = metric,
                   samplingmethod = samplingmethod,
                   numberFOLD = numberFOLD,
                   numberFoldRepeats = numberFoldRepeats,
                   searchMethod = searchMethod,
                   outer_method = outer_method,
                   learner=learner,
                   VariablesearchAlgorithm = VariablesearchAlgorithm,
                   combined_variables = combined_variables,
                   clinical_variables_names = clinical_variables_names,
                   nested_features_selection_search = nested_features_selection_search,
                   TMB_variables_names = TMB_variables_names), dots)
    
    do.call(nestcv_innertrain, args)
    
  }
  
  if (cv.cores >= 2) { 
    
    if (Sys.info()["sysname"] == "Windows") {
      
      start.time <- Sys.time()
      
      cl <- parallel::makePSOCKcluster(cv.cores)
      
      doParallel::registerDoParallel(cl)
      
      outer_res <- foreach(i=outer_folds, .packages = c("Rfast", "caret") ,
                           
                                                     .export = c(varlist,
                                                      "model_gridsParameters",
                                                      "createMultiFolds",
                                                      "twoClassSummary",
                                                      "prerformance_metric",
                                                      "trainControl",
                                                      "train",
                                                      'wrapperEvaluator',
                                                      'trainclassificationFunc',
                                                      "Nestedmodel_FinalgridsParam",
                                                      'randomsample',
                                                      'ttest_filter',
                                                      'filter_end',
                                                      "collinear",
                                                      "sequentialBackwardSearch")) %dopar% parallelInnerNested(i)
      
      registerDoSEQ()
      
      parallel::stopCluster(cl)
      
      end.time <- Sys.time()
      
      difference <- difftime( end.time, start.time, units='mins')
      
      #print(difference)
      
    }else {
      
      start.time <- Sys.time()
      
      cl <- parallel::makePSOCKcluster(cv.cores)
      
      doParallel::registerDoParallel(cl)  # this with the uncommented outer_res
      
      outer_res <- foreach(i=outer_folds, .packages = c("Rfast", "caret"),
                           
                                                       .export = c(varlist,
                                                      "model_gridsParameters",
                                                      "createMultiFolds",
                                                      "twoClassSummary",
                                                      "prerformance_metric",
                                                      "trainControl",
                                                      "train",
                                                      'wrapperEvaluator',
                                                      'trainclassificationFunc',
                                                      "Nestedmodel_FinalgridsParam",
                                                      'randomsample',
                                                      'ttest_filter',
                                                      'filter_end',
                                                      "collinear",
                                                      "sequentialBackwardSearch")) %dopar% parallelInnerNested(i)
      
      registerDoSEQ()
      
      parallel::stopCluster(cl)
      
      end.time <- Sys.time()
      
      difference <- difftime( end.time, start.time, units='mins')
      
      #print(difference)
      
    }
    
    
  } else {
    
    start.time <- Sys.time()
    
    outer_res <- mclapply(outer_folds,
                          parallelInnerNested,
                          mc.cores = 1)
    
    end.time <- Sys.time()
    difference <- difftime( end.time, start.time, units='mins')
    #print(difference)
    
  }
  
  #train_predslist <- lapply(outer_res, '[[', 'train_preds')
  #test_predslist <- lapply(outer_res, '[[', 'test_preds')
  
  ## train test values
  train_metrics = as.numeric(lapply(outer_res, '[[', 'train_metric'))
  test_metrics = as.numeric(lapply(outer_res, '[[', 'test_metric'))
  abs_train_test_dfference = abs( train_metrics - test_metrics)
  
  # Best result for the selected metric
  #mean_test_metric_result <- mean(test_metrics)
  
  
  ###### selected Features
  selectedFeatures_all = lapply(outer_res, '[[', 'selectedFeatures')
  
  ## consensus features is extrememly parsimonuos  -- see here: https://academic.oup.com/bioinformatics/article/36/10/3093/5716331
  #final_Features <- Reduce(intersect, selectedFeatures_all )
  
  features_occurence = table(unlist( selectedFeatures_all))
  featurew_prop_occurence = features_occurence/length(outer_folds)                                
 
  # retain features with more than 50 percent occurence  -- see here https://www.tandfonline.com/doi/full/10.1080/03610918.2020.1850790
  final_Features = names(features_occurence[featurew_prop_occurence > 0.50])
  
  
  ### reduced featrues space
  new_train_data = resmaple_train_data[, c(class, final_Features)]


  ##########################################
  ## model final grid parameters
  if (finalCV) {
    # use CV on whole data to finalise parameters
    if (cv.cores >= 2) {
      if (Sys.info()["sysname"] == "Windows") {
        cl <- makeCluster(cv.cores)
        registerDoParallel(cl)
        on.exit({stopCluster(cl)
          foreach::registerDoSEQ()})
      } else {
        # unix
        registerDoParallel(cores = cv.cores)
        on.exit(foreach::registerDoSEQ())
      }
    }
    
    bestTunes = NULL
    
    ctrl2 <- do.call(caret::trainControl, resamplingParams)
    
    set.seed(1234)
    
    final_fit <-  do.call(caret::train, append(list(
      form = as.formula(paste( class, ".", sep = "~")),
      data = new_train_data,
      method = learner,
      trControl = ctrl2),
      fittingParams))
    
  } else {
    
    bestTunes <- lapply(outer_res, function(i) i$fit$bestTune)
    bestTunes <- as.data.frame(data.table::rbindlist(bestTunes))
    rownames(bestTunes) <- paste('Fold', seq_len(nrow(bestTunes)))
    
    if((learner == 'lda')|(learner == 'bayesglm')){
      best_tune <- NA
    }else{
      best_tune <- finaliseTune(bestTunes)
    }
    
    # use outer folds for final parameters, fit single final model
    finalTune <- finaliseTune(bestTunes)
  
    
    Nestedcaretparameters  = Nestedmodel_FinalgridsParam(new_train_data,
                                                         learner = learner, 
                                                         metric = metric,
                                                         samplingmethod = samplingmethod, 
                                                         numberFOLD = numberFOLD, 
                                                         numberFoldRepeats = numberFoldRepeats,
                                                         searchMethod = searchMethod,
                                                         finalTune = finalTune)
    
    fittingParams =  Nestedcaretparameters$fittingParams
    resamplingParams =  Nestedcaretparameters$resamplingParams
    
    ctrl2 <- do.call(caret::trainControl,resamplingParams)
    
    set.seed(1234)
    
    final_fit <-  do.call(caret::train, append(list(
      form = as.formula(paste( class, ".", sep = "~")),
      data = new_train_data,
      method = learner,
      trControl = ctrl2),
      fittingParams))
    
  }
  
  
  
  #####################
  res <- list(NULL)
  
  new_train_data2 = train_data[, c(class, final_Features)]
  
  best.set.aux <- matrix(rep(0,(ncol(resmaple_train_data)-1)), ncol=(ncol(resmaple_train_data)-1), byrow=FALSE, dimnames=list(c(),column.names[-class.position]))
  best.set.aux[which(column.names[-class.position]%in%final_Features)] <- 1
  
  
  best_tune = final_fit$bestTune
  rowBestTune <- as.numeric(rownames(final_fit$bestTune))
    # Best result for the selected metric
  best_metric_result = final_fit$results[rowBestTune,final_fit$metric]
  
  
  ## confusion matrix
  set.seed(1234)
  predicted = predict(final_fit, newdata = new_train_data2)
  predictions = factor(predicted, levels = levels(as.factor(new_train_data2[,class])))
  confusionmatrix = confusionMatrix(as.factor(predictions), as.factor(new_train_data2[,class]))
  

  res[[1]] <- best.set.aux
  res[[2]] <- final_Features
  res[[3]] <- best_tune
  res[[4]] <-    best_metric_result 
  res[[5]] <-   final_fit
  res[[6]] <-  confusionmatrix
  res[[7]]   <-  train_metrics
  res[[8]]   <-  test_metrics
  res[[9]]   <- selectedFeatures_all
  res[[10]]  <- bestTunes
  

  names(res) <- c("bestFeaturesID", 
                  'bestFeaturesnames',
                  'best parameter',
                  "bestFitness", 
                  'best model',
                  'best confusion matrix',
                  'outer train perfor values',
                  'outer test perfor values',
                  'selected features accross inner folds',
                  'tuned-parameter dataframe'
  ) 
  
  
  result =  list(best_model = res,
                 table_all_models_summaries = NULL, 
                 list_all_models =  NULL)
  
  
  return(result)
}





nestcv_innertrain <- function(class, 
                             train_data,
                             testIndices,
                             metric,
                             samplingmethod, 
                             numberFOLD, 
                             numberFoldRepeats,
                             searchMethod,
                             outer_method,
                             learner,
                             VariablesearchAlgorithm,
                             combined_variables,
                             clinical_variables_names,
                             nested_features_selection_search = TRUE,
                             TMB_variables_names) {
  
  
  if(outer_method =='SKFold'){
    
    train_data2 = train_data[testIndices,]
    test_data = train_data[-testIndices,]
    
  }else{
    train_data2 = train_data[-testIndices,]
    test_data = train_data[testIndices,]
  }
  


  ########################### apply filter: combinations of wilcoxon and correlation to reduce collinear
  if(nested_features_selection_search){
    
    if( (combined_variables == 'clinical') | (combined_variables == 'clinical+TMB')) {
      
      filt_xtrain = train_data2
      filt_xtest  = test_data
      
    }else{
      
      df_x = train_data2[, -which(names(train_data2) %in% c('response', clinical_variables_names, TMB_variables_names))]
      
      args <- list(y = train_data2[,class], 
                   x = df_x,
                   p_cutoff = 0.01, 
                   rsq_cutoff = 0.85^2, 
                   type = "names")
      
      fset <- do.call(ttest_filter, args)
      
      filt_xtrain <- train_data2[, c(class, clinical_variables_names, fset, TMB_variables_names)]
      filt_xtest <- test_data[, c(class, clinical_variables_names, fset, TMB_variables_names), drop = FALSE]
      
    }
    
  }else{
    
    filt_xtrain = train_data2
    filt_xtest  = test_data
    
  }
  
  
  ### balance out the training part
  resmaple_out <- randomsample(y = filt_xtrain[,class],
                               x = filt_xtrain[, -which(names(filt_xtrain) %in% c(class))]  )

  resmaple_y2 <- as.data.frame(resmaple_out$y)
  colnames(resmaple_y2) = class
  resmaple_x2 <- resmaple_out$x

  filt_resmaple_train_data2 = cbind(resmaple_y2, resmaple_x2)
  
  set.seed(1234)
  caretparameters = model_gridsParameters(filt_resmaple_train_data2,
                                          learner=learner, 
                                          metric=metric,
                                          samplingmethod=samplingmethod, 
                                          numberFOLD=numberFOLD, 
                                          numberFoldRepeats=numberFoldRepeats,
                                          searchMethod=searchMethod)

  fittingParams = caretparameters$fittingParams
  resamplingParams = caretparameters$resamplingParams
  
  evaluatorInternal <- wrapperEvaluator(learner=learner,
                                         resamplingParams = resamplingParams,
                                         fittingParams = fittingParams)
   
  ########################### apply filter: combinations of wilcoxon and correlation to reduce collinear
  if(nested_features_selection_search){
    
    # final training model
    inner_modelingPart = trainclassificationFunc(filt_resmaple_train_data2,
                                                 filt_xtest,
                                                 class = class,
                                                 evaluator = evaluatorInternal,
                                                 VariablesearchAlgorithm = VariablesearchAlgorithm,
                                                 learner = learner,
                                                 metric = metric,
                                                 samplingmethod = samplingmethod,
                                                 numberFOLD = numberFOLD,
                                                 numberFoldRepeats = numberFoldRepeats,
                                                 searchMethod = searchMethod,
                                                 combined_variables = combined_variables)
    inner_modelingPart = inner_modelingPart$best_model  
    
    fit = inner_modelingPart$'best model'
    
    ### filter wrapper dataset
    
    filt_wrapper_xtrain = filt_resmaple_train_data2[, c(class, inner_modelingPart$bestFeaturesnames)]
    filt_wrapper_xtest = filt_xtest[, c(class, inner_modelingPart$bestFeaturesnames)]
    
    ## selected features
    selectedFeatures =  inner_modelingPart$bestFeaturesnames
    
    ## model parameter
    initial_param = inner_modelingPart$`best parameter`
    
  }else{
    
    column.names <- names(filt_resmaple_train_data2) 
    class.position <- which(column.names == class) 
    features <- column.names[-class.position] 
  
    inner_modelingPart <- evaluatorInternal(filt_resmaple_train_data2, class, features)  #### here
   
    fit = inner_modelingPart$model
    
    ### filter wrapper dataset
    
    filt_wrapper_xtrain = filt_resmaple_train_data2
    filt_wrapper_xtest = filt_xtest

    ## selected features
    selectedFeatures =   features #inner_modelingPart$featuresnames
    
    ## model parameter
    initial_param = inner_modelingPart$best_tune
    
  }
  

  ## train
   train_predy = predict(fit, newdata = filt_wrapper_xtrain)
   train_preds <- data.frame(predy=train_predy, testy=filt_wrapper_xtrain[, class])
   
   if (is.factor(filt_wrapper_xtrain$response)) {
     train_predyp <- predict(fit, newdata = filt_wrapper_xtrain, type = "prob")
     # note predyp has 2 columns
     train_preds$train_predyp <- train_predyp[,2]
   }
   
   rownames(train_preds) <- rownames(filt_wrapper_xtrain)
   
   
  
  ### test
  test_predy = predict(fit, newdata = filt_wrapper_xtest)
  test_preds <- data.frame(predy=test_predy, testy=filt_wrapper_xtest[, class])
  
  if (is.factor(filt_wrapper_xtrain$response)) {
    test_predyp <- predict(fit, newdata = filt_wrapper_xtest, type = "prob")
    # note predyp has 2 columns
    test_preds$test_predyp <- test_predyp[,2]
  }
  
  rownames(test_preds) <- rownames(filt_wrapper_xtest)
  
  
  ### test and train performance
  metric_test = prerformance_metric(fit, filt_wrapper_xtest, class)
  metric_train = prerformance_metric(fit, filt_wrapper_xtrain, class)

  
  result <- list(inner_modelingPart =  inner_modelingPart,
                 train_preds = train_preds,
                 test_preds = test_preds,
               fit = fit,
              test_metric = metric_test,
             train_metric = metric_train,
             selectedFeatures = selectedFeatures,
             innner_param = initial_param)
  
  return(result)
}






# finalise the caret model tuning from bestTune dataframe
#' @importFrom stats median
finaliseTune <- function(x) {
  fintune <- lapply(colnames(x), function(i) {
    if (is.numeric(x[, i])) return(median(x[, i]))
    tab <- table(x[, i])
    names(tab)[which.max(tab)]  # majority vote for factors
  })
  names(fintune) <- colnames(x)
  data.frame(fintune, check.names = FALSE)
}



### get model metrics
prerformance_metric = function(model, data, class){
  test_data = data
  fit = model
  
  predy = predict(fit, newdata = test_data)
  
  test_confusionmatrixSUMMARY = confusionMatrix(as.factor(predy),
                                                as.factor(test_data[, class]))
  if(metric == "Kappa"){
    metric_test = test_confusionmatrixSUMMARY$overall['Kappa']
  }else if(metric == 'Sens'){
    metric_test= test_confusionmatrixSUMMARY$byClass['Sensitivity']
  }else if (metric == 'Spec'){
    metric_test= test_confusionmatrixSUMMARY$byClass['Specificity']
  }else if (metric == 'ROC'){
    test_predictionsProb <- predict(fit, newdata = test_data, type="prob")
    test_ROC <- pROC::roc(test_data[,class],
                          test_predictionsProb[,test_confusionmatrixSUMMARY$positive],
                          quiet = TRUE, ci=TRUE,
                          direction=">")
    test_auc = pROC::auc(test_ROC)
    metric_test =  as.numeric(test_auc)
  }
  return(metric_test)
}





## nested model final grid parameters
Nestedmodel_FinalgridsParam = function(data,
                                       learner, 
                                       metric,
                                       samplingmethod, 
                                       numberFOLD, 
                                       numberFoldRepeats,
                                       searchMethod,
                                       finalTune){
  
  tuneLength = 2000
  
  if(learner == 'glmnet'){
    
    grids = expand.grid(alpha =  finalTune$alpha, 
                        lambda =  finalTune$lambda)
    
    fittingParams <- list(metric = metric,
                          
                          family = "binomial",
                          
                          importance = TRUE,
                          
                          standardize = FALSE,
                          
                          tuneGrid = grids, 
                          
                          tuneLength = tuneLength)
    
    
  }else if (learner == 'lda'){
    
    grids = NULL
    fittingParams = list(metric = metric,
                         tuneLength = tuneLength)
    
  }  else if (learner == 'bayesglm'){
    
    grids = NULL
    fittingParams = list(metric = metric,
                         tuneLength = tuneLength)
    
  }else if (learner == 'svmRadial'){
    
    grids =  expand.grid(sigma = finalTune$sigma,
                         C = finalTune$C)
    
    fittingParams = list(metric = metric,
                         tuneGrid = grids,
                         prob.model = TRUE,
                         tuneLength = tuneLength)
    
  }else if(learner ==  'svmLinear'){
    
    grids =  expand.grid(C = finalTune$C)
    
    fittingParams = list(metric = metric,
                         tuneGrid = grids,
                         prob.model = TRUE,
                         tuneLength = tuneLength)
    
    
  }else if(learner ==  'knn'){
    
    
    grids =  expand.grid(k = finalTune$k)
    
    fittingParams = list(metric = metric,
                         tuneGrid = grids,
                         tuneLength = tuneLength)
    
    
  }else if (learner == 'nnet'){
    
    grids = expand.grid(decay = finalTune$decay ,
                        size = finalTune$size )
    
    fittingParams = list(metric = metric,
                         tuneGrid = grids,
                         tuneLength = tuneLength,
                         trace = FALSE,
                         verbosity = 0)
    
  } else if (learner == "rf"){
    
    grids <- expand.grid(mtry =  finalTune$mtry)
    
    fittingParams = list(metric = metric,
                         tuneGrid = grids,
                         tuneLength = tuneLength,
                         importance=TRUE,
                         ntree=1500)
    
    
  }else if (learner == 'rda'){
    grids = expand.grid(gamma = finalTune$gamma, 
                        lambda =  finalTune$lambda) 
    
    fittingParams = list(metric = metric,
                         tuneGrid = grids,
                         tuneLength = tuneLength)
    
  }
  
  
  if(samplingmethod == 'LOOCV'){
    
    numberFOLD= dim(data)[1] ;  numberFoldRepeats = 1
    
  } else if((samplingmethod == "boot632") | (samplingmethod == "upSampleboot632") | (samplingmethod == "downsampledboot632") | (samplingmethod == "roseboot632") ){
    
    numberFOLD= numberFOLD ;  numberFoldRepeats =  1
    
  } else{
    
    numberFOLD= numberFOLD ;  numberFoldRepeats =  numberFoldRepeats
  }
  
  
  if ((learner ==  'lda') | (learner == 'bayesglm')) {
    ngridsDim= 1
    
  }else {
    
    ngridsDim= dim(grids)[1]
  }
  
  
  
  if((samplingmethod == "boot632") | (samplingmethod == "upSampleboot632") | (samplingmethod == "downsampledboot632") | (samplingmethod == "roseboot632") ) {
    
    set.seed(1234)
    seeds <- vector(mode = "list", length = (numberFOLD*numberFoldRepeats)+2)
    for(i in 1:((numberFOLD*numberFoldRepeats)+1) ) seeds[[i]] <- sample.int(1000000, ngridsDim)
    ## For the last model:
    seeds[[((numberFOLD*numberFoldRepeats)+2)]] <- sample.int(1000000, 1)
    
  }else{
    
    set.seed(1234)
    seeds <- vector(mode = "list", length = (numberFOLD*numberFoldRepeats)+1)
    for(i in 1:(numberFOLD*numberFoldRepeats)) seeds[[i]] <- sample.int(1000000, ngridsDim)
    ## For the last model:
    seeds[[((numberFOLD*numberFoldRepeats)+1)]] <- sample.int(1000000, 1)
    
  }
  
  
  if((samplingmethod == 'stratifiedRepeatedCV')| (samplingmethod =='upSamplestratifiedRepeatedCV') | (samplingmethod == 'downsampledstratifiedRepeatedCV')| (samplingmethod == 'rosestratifiedRepeatedCV')  ){
    
    samplingmethod2 = 'repeatedcv'
    
    set.seed(1234)
    cvIndex <- createMultiFolds(data$response,
                                k = numberFOLD,
                                times = numberFoldRepeats)
    
    
  } else if((samplingmethod == "boot632") | (samplingmethod == "upSampleboot632") | (samplingmethod == "downsampledboot632") | (samplingmethod == "roseboot632")){
    
    samplingmethod2 = "boot632"
    cvIndex = NULL
    
  }else{
    
    samplingmethod2 =  samplingmethod
    cvIndex = NULL
    
  }
  
  
  if( (samplingmethod == "downsampledboot632") |  (samplingmethod == 'downsampledstratifiedRepeatedCV') ){
    
    sampling = 'down'
    
  }else if ((samplingmethod == "upSampleboot632") | (samplingmethod =='upSamplestratifiedRepeatedCV')){
    
    sampling = 'up'
    
  }else if ((samplingmethod == "roseboot632") | (samplingmethod == 'rosestratifiedRepeatedCV') ){
    
    sampling = 'rose'
    
  }else {
    
    sampling = NULL
  }
  
  
  if((metric == 'Accuracy') | (metric == 'Kappa') ){
    
    if((learner ==  'svmLinear')| (learner == 'svmRadial')){
      
      resamplingParams <- list(index = cvIndex,
                               
                               method = samplingmethod2, 
                               
                               number = numberFOLD,
                               
                               repeats = numberFoldRepeats,
                               
                               verbose = FALSE, 
                               
                               classProbs = TRUE,
                               
                               search = searchMethod,
                               
                               savePredictions = "final",
                               
                               allowParallel = TRUE,
                               
                               sampling = sampling,
                               
                               seeds = seeds)
      
    }else{
      
      resamplingParams <- list(index = cvIndex,
                               
                               method = samplingmethod2, 
                               
                               number = numberFOLD,
                               
                               repeats = numberFoldRepeats,
                               
                               verbose = FALSE, 
                               
                               classProbs = TRUE,
                               
                               search = searchMethod,
                               
                               savePredictions = "final",
                               
                               allowParallel = TRUE,
                               
                               sampling = sampling,
                               
                               seeds = seeds)
    }
  }else{
    if((learner ==  'svmLinear')| (learner == 'svmRadial')){
      
      resamplingParams <- list(index = cvIndex,
                               
                               method = samplingmethod2, 
                               
                               number = numberFOLD,
                               
                               repeats = numberFoldRepeats,
                               
                               verbose = FALSE, 
                               
                               classProbs = TRUE,
                               
                               summaryFunction = twoClassSummary,
                               
                               search = searchMethod,
                               
                               savePredictions = "final",
                               
                               allowParallel = TRUE,
                               
                               sampling = sampling,
                               
                               seeds = seeds)
      
    }else{
      
      resamplingParams <- list(index = cvIndex,
                               
                               method = samplingmethod2, 
                               
                               number = numberFOLD,
                               
                               repeats = numberFoldRepeats,
                               
                               verbose = FALSE, 
                               
                               classProbs = TRUE,
                               
                               summaryFunction = twoClassSummary,
                               
                               search = searchMethod,
                               
                               savePredictions = "final",
                               
                               allowParallel = TRUE,
                               
                               sampling = sampling,
                               
                               seeds = seeds)
    }
  }
  
  list(learner = learner,
       fittingParams = fittingParams,
       resamplingParams = resamplingParams)
}



